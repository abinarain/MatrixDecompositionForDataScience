# -*- coding: utf-8 -*-
"""MDAProject1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xkc2Ye6Sq7740E_EWsvz6Sscm7OSUW92
"""

#Author: Abhishek N. Singh
#Student Number: 322079
#Assignment Project 1
#Course: Matrix Decomposition for Data Science
#Date: 2nd February 2021
#Description: We do analysis of SVD and PCA

import numpy as np
import scipy
from numpy.linalg import svd
from sklearn.cluster import KMeans
import cartopy.crs as ccrs
import matplotlib.pyplot as plt
import matplotlib
import networkx as nx
from networkx.algorithms import bipartite
## Task 1

#########

## Load the data. We could use pandas to get tables, but as we're just doing
## matrix stuff numpy arrays are fine. 
data = np.genfromtxt('worldclim.csv', delimiter=',', skip_header=1)
coord = np.genfromtxt('coordinates.csv', delimiter=',', skip_header=1)
lat = coord[:,0]
lon = coord[:,1]

"""**Note** the installation of packages codes are below"""

!pip install --no-binary shapely shapely --force

data

coord

lat

lon





#!apt-get -qq install python-cartopy python3-cartopy
!apt-get -V -y -qq install python-cartopy python3-cartopy

!pip uninstall shapely -y

!pip install shapely --no-binary shapely

"""**Task 1**: Normalization
Download the data and utility files (assignment1.zip) from Moodle. That package contains files
assignment1.{R,m,py}, data files worldclim.{csv,txt} and coordinates.{csv,txt}, meta data file
worldclim_attributes.txt, and some helper files (utils.R and matching.m). You can fill your code to
the appropriate assignment1 file (referred as the file henceforth) and return it as a part of your solution.
Follow the steps in the file to load the worldclim data. This data contains information about the
bioclimatic conditions (minimum, maximum, and average temperature in tenths of degrees Celcius and
average precipitation in millimetres) per months in the world. Spend some time exploring the data (you
do not need to report these explorations, though). Compute the SVD of the data, and plot the first two left
singular vectors to a map. This is also explained in the provided file. Can you interpret the results?
Play around with different color schemes and markers. Can you make the results more interpretable that
way? Does the color scheme affect the interpretation?
Normalize the data to z-scores. Implement the normalization yourself (i.e. do not use any build-in
zscore functions). Given the type of data we have, do you think this normalization is sensible?
Compute the SVD of the normalized data, and plot again the first two left singular vectors. Have they
changed? Has your interpretation changed? Why?
"""

## Compute SVD of data
U, S, V = svd(data, full_matrices=False, compute_uv=True)

U.shape, S.shape, V.shape

U

S

V

#Lets construct the SIGMA matrix from S
SigmaM = np.diag(S)

SigmaM

U[:,1] #Second left singular vector

U[:,0] #First left singular vector





## Plot the base map; nothing is shown yet
ax = plt.axes(projection=ccrs.PlateCarree())
ax.set_global()
ax.coastlines()
#ax.plot(lon, lat, color='red', transform=ccrs.PlateCarree())
## If you want to see how it looks, write
plt.show()

## Plot the first column of U so that the color indicates the value
##This is the first left singular vector
plt.scatter(lat, lon, s=1, c = U[:,0], cmap='RdYlBu')
plt.show()

## Plot the first column of U so that the color indicates the value
##This is the first left singular vector
plt.scatter(lat, lon, s=1, c = U[:,0])
plt.show()



## Plot the second column
#This is the 2nd singular vector
ax = plt.axes(projection=ccrs.PlateCarree())
ax.set_global()
ax.coastlines()
plt.scatter(lat, lon, s=1, c = U[:,1])
plt.colorbar(shrink=0.5)
plt.show()

#Comments
#We know that the first column of U is more important than 2nd column of U and so on and so forth, in describing the data matrix. 
# In other words, the left singular vector 1 is more important than the left singular vector 2, and the importance is tapped
# in the S matrix 2.67803546e+05 being importance factor for 1st left singular vector and 1.24366268e+05 being the 
# importance factor for 2nd left singular vector. For instance the first singular vector could be tapping in the precipitation distribution
# as we can see by the similarity in the color in the geographies with similar longitudes. The second left singular vector can
# perhaps be tapping the temperature as per latitude.

#Lets try for the 3rd left singular vector. We see a totally different color, which perhaps represents darker color for lower precipitation
ax = plt.axes(projection=ccrs.PlateCarree())
ax.set_global()
ax.coastlines()
plt.scatter(lat, lon, s=1, c = U[:,2], cmap='RdYlBu')
plt.colorbar(shrink=0.5)
plt.show()

#Lets try for the 47th left singular vector. We see a almost no color, representing that the column is insignificant
ax = plt.axes(projection=ccrs.PlateCarree())
ax.set_global()
ax.coastlines()
plt.scatter(lat, lon, s=1, c = U[:,47], cmap='RdYlBu')
plt.colorbar(shrink=0.5)
plt.show()

#Comment
#Thus the color scheme does not seem to be impacting much of interpretation. But the vector chosen does matter.

#Normalize the matrix data
xmax, xmin = data.max(), data.min()
x = (data - xmin)/(xmax - xmin)
print("After normalization:")
print(x)

## Compute SVD of normalized data
Un, Sn, Vn = svd(data, full_matrices=False, compute_uv=True)

## Plot the first column of U so that the color indicates the value
##This is the first left singular vector
plt.scatter(lat, lon, s=1, c = Un[:,0])
plt.show()

## Plot the first column of U so that the color indicates the value
##This is the first left singular vector
plt.scatter(lat, lon, s=1, c = U[:,0])
plt.show()

#We can see that the normalized data does lead to some amount of differences in the color scheme. Thus, normalization should be preferred to avoid biases.

#Z-score normalization by centering :
#The z-scores are attributes whose values are transformed by
#centering them to 0
# Remove the mean of the attribute’s values from each value
# normalizing the magnitudes
# Divide every value with the standard deviation of the attribute

ZnormData = (data - np.mean(data))/(np.std(data))
print("After normalization:")
print(ZnormData)

## Compute SVD of Z-normalized data
Uzn, Szn, Vzn = svd(ZnormData, full_matrices=False, compute_uv=True)

## Plot the first column of U so that the color indicates the value
##This is the first left singular vector
plt.scatter(lat, lon, s=1, c = Uzn[:,0])
plt.show()

plt.scatter(lat, lon, s=1, c = Uzn[:,1])
plt.show()

#The z-score normalization does lead to a different plot.  z-scores assume that all attributes are equally important
# attribute values are approximately normally distributed
#Main problem in task 1 was to install cartopy module.

#Task 2
## If S is an array that contains the singular values, you can plot them by
#plt.plot(S)
#plt.show()

"""**Task 2**: Selecting the rank
In this task, we use the normalized worldclim data that you did in the previous task. Compute the SVD
of the data. Implement the following rank selection methods and use them to decide what would be a good
rank for the truncated SVD:
• Guttman–Kaiser criterion
• 90% of explained Frobenius norm
• Scree test
• Entropy-based method
• Random flipping of signs
Report the rank each method suggests (and when subjective evaluation is needed to decide the rank,
explain why you choose that rank). Discuss the results: are they same or different? Why? Do you think
some methods work better than others? Why? What rank would you choose? Why?

"""

#Guttman–Kaiser criterion:
#Select k so that for all i > k, σi < 1
#Motivation: all components with singular value less than unit are
#uninteresting

# select enough singular values such
#that the sum of their squares is 90% of the total sum of the squared
#singular values
# The exact percentage can be different (80%, 95%)
# Motivation: The resulting matrix “explains” 90% of the Frobenius
#norm of the matrix (a.k.a. energy)
Szn

#Lets construct the SIGMA matrix from Szn
SigmaMz = np.diag(Szn)

SigmaMz

#SigmaMz.dtype



diagnoalElementsSigma = np.diag(SigmaMz)

for k in range(0,len(diagnoalElementsSigma)):
  if diagnoalElementsSigma[k] < 1:
    print(k)
    break

#Thus we are able to get the index which corresponds to RANK of the SVD truncated matrix which is 36+1 = 37.
#Since we start from index 0, thus the 37th column is the relevant rank column.

# This method is based on arbitrary thresholds and do not consider the “shape” of the data

ForbeniusNormSquared = np.trace(SigmaMz.dot(SigmaMz.T))

ForbeniusNormSquared

sum = 0
NinetyPcForbenNoSq = 0.90 * ForbeniusNormSquared
for k in range(0,len(diagnoalElementsSigma)):
  sum = sum + np.square(diagnoalElementsSigma[k])
  if sum >= NinetyPcForbenNoSq:
    print(k)
    break

print(sum)

#Thus by this method the rank is equal to 1+1 = 2. i.e. we choose 2 columns.
#More details of theory at http://resources.mpi-inf.mpg.de/d5/teaching/ss13/dmm/slides/03-svd-handout.pdf

#Cattell’s Scree test





#Code taken from: https://stats.stackexchange.com/questions/12819/how-to-draw-a-scree-plot-in-python 
eigvals = Szn**2 / np.sum(Szn**2)  #

num_vars = Uzn.shape[1]
num_obs = Uzn.shape[0]

#fig = plt.figure(figsize=(8,5))
sing_vals = np.arange(num_vars) + 1
plt.plot(sing_vals, eigvals, 'ro-', linewidth=2)
plt.title('Scree Plot')
plt.xlabel('Singular Value Sigma Vectors')
plt.ylabel('Eigenvalue')
leg = plt.legend(['Eigenvalues from SVD'], loc='best', borderpad=0.3,shadow=False, prop=matplotlib.font_manager.FontProperties(size='small'), markerscale=0.4)
plt.show()

# Sree plots are subjective. We can see that there is a sharp dip at 6th data point after which the curve is mostly flat.
# Thus, the rank by this method can be chosen to be 6



#Entropy method

len(diagnoalElementsSigma)

# http://resources.mpi-inf.mpg.de/d5/teaching/ss13/dmm/slides/03-svd-handout.pdf Here I get the details of Entropy

EntropyMultiplier = -(1 / np.log2(len(diagnoalElementsSigma)))
Entropy = 0
for k in range(0,len(diagnoalElementsSigma)):
  fk = np.square(diagnoalElementsSigma[k]) / ForbeniusNormSquared #Relative contribution of each singular value to overall Forbenius norm
  Entropy = Entropy + ( fk * np.log2(fk) )
Entropy = EntropyMultiplier * Entropy
print(Entropy)

fSum = 0
for k in range(0,len(diagnoalElementsSigma)):
  fk = np.square(diagnoalElementsSigma[k]) / ForbeniusNormSquared
  fSum = fSum + fk
  if fSum >= Entropy:
    print(k)
    break

#As the entropy is close to 0 than to 1, almost all the mass lies towards the initial singular values
#This is also confirmed by the fact that the smallest k for which sum of frequency exceeds entropy is greater than
#entropy is 0. Thus the rank by Entropy method will be 0 +1, which mean 1st column only.

#Random flipping of signs
# Source: http://resources.mpi-inf.mpg.de/d5/teaching/ss13/dmm/slides/03-svd-handout.pdf

#Multiply every element of the data A randomly with either 1 or −1 to
#get A˜
#I The Frobenius norm doesn’t change (kAkF = kA˜kF )
#I The spectral norm does change (kAk2 6= kA˜k2)
#F How much this changes depends on how much “structure” A has

#The residual matrix contains the last m − k columns of U,
#min{n, m} − k singular values, and last n − k rows of V

#T
#I If A−k is the residual matrix of A after rank-k truncated SVD and A˜ −k
#is that for the matrix with randomly flipped signs, we select rank k to
#be such that (kA−k k2 − kA˜ −k k2)/kA−k kF is small

#Here k can be as small as 2 which can be the rank

#I would go with the rank of the Scree plot i.e. 6. This is because we can look at the shape of data and can see where
# exactly does it matter to take relevant singular vectors.



#Task 3

"""**Task 3**: Clustering and PCA
For this task, our goal is to cluster the rows of the data into five clusters and visualize the result. One
way to do this is to first cluster the data into five clusters using the k-means algorithm, and then plot the
data points into the map in such a way that the color of the marker identifies the cluster (the provided file
explains the process).
Look at the resulting clustering and explain what the clusters may represent (remember, the data contains
temperature and rainfall information).
For another visualization of the results, plot the data so that the x-axis position comes from the first left
singular vector, the y-axis position comes from the second left singular vector, and the color of the marker is
defined by the clustering.
Are the clusters well-separated from each other in the plot or are they mixed? Do some of the clusters
look like outliers?
Now, apply the Karhunen–Loève transformation (i.e. PCA) to project the data into a 2-dimensional
subspace. Repeat the clustering and visualization steps with this new data. Did the results change? Why do
you think the results changed or did not change?

"""

## We again use the normalized data. If that is contained in matrix 'D'
## we can compute the k-means to 5 clusters with 10 re-starts as
clustering = KMeans(n_clusters=5, n_init=10).fit(ZnormData)
idx = clustering.labels_

## The plotting 
ax = plt.axes(projection=ccrs.PlateCarree())
ax.set_global()
ax.coastlines()
plt.scatter(lat, lon, s=1, c = idx, cmap='tab10')
plt.show()

#Clearly, the data may contain representation of area havig similar temperatures. Rainfall seems to have little
#effect here, given that the Indian subcontinent has been given the same cluster as that of middle east, whereas we 
#know for sure by ground reality that the precipitation levels are different for these two regions.

## The plotting of the data into 5 clusters with the x-axis and y-axis as 
#1st left singular vector and 2nd left singular vector
#
plt.scatter(Uzn[:,0], Uzn[:,1], s=1, c = idx, cmap='tab10')
plt.show()

#We note that the clusters are not well separated. The borders of each of of them 
#touch with other clusters. Clearly, K-Means does not looks to be the best 
#clustering method here. There is no clear-cut outlier though.

from sklearn.decomposition import PCA # This is for Karhunen–Loève transformation
pca = PCA(n_components=2)

principalComponents = pca.fit_transform(ZnormData)

#principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])

principalComponents[:,0]

#Plotting Data by Principal components - 1st and 2nd

plt.scatter(principalComponents[:,0], principalComponents[:,1], s=1, c = idx, cmap='tab10')
plt.show()

#There seems to be LITTLE to almost no difference among the plots. This says that, taking the 
# left singular vectors in SVD decomposition corresponds to that of conducting PCA.

#SVD is one way to get PCA, and this is illustrated by the above example.

#Tasks finished. Note, some libraries such as NetworkX were not used at all.